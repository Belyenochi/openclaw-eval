#!/usr/bin/env python
"""
é›†æˆæµ‹è¯•ï¼šéªŒè¯ judge å‘½ä»¤çš„å®Œæ•´æµç¨‹

è¿™ä¸ªæµ‹è¯•ä¸ä¼šå®é™…è°ƒç”¨ Anthropic APIï¼Œè€Œæ˜¯éªŒè¯ï¼š
1. å‘½ä»¤è¡Œå‚æ•°è§£æ
2. æŠ¥å‘Šæ–‡ä»¶è¯»å–
3. æ•°æ®ç»“æ„éªŒè¯
4. è¾“å‡ºæ–‡ä»¶ç”Ÿæˆï¼ˆä½¿ç”¨ mock æ•°æ®ï¼‰
"""

import json
import sys
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_judge_workflow():
    """æµ‹è¯• judge å‘½ä»¤çš„å®Œæ•´å·¥ä½œæµ"""

    print("=" * 60)
    print("EDD Judge Command - é›†æˆæµ‹è¯•")
    print("=" * 60)

    # 1. åˆ›å»ºæµ‹è¯•æŠ¥å‘Š
    print("\n[1/5] åˆ›å»ºæµ‹è¯•æŠ¥å‘Š...")
    test_report = [
        {
            "case": {
                "id": "test_list_files",
                "message": "åˆ—å‡ºå½“å‰ç›®å½•çš„æ–‡ä»¶"
            },
            "tool_names": ["Bash"],
            "final_output": "å½“å‰ç›®å½•åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š\n- README.md\n- pyproject.toml\n- src/",
            "passed": True,
            "duration_s": 1.2
        },
        {
            "case": {
                "id": "test_read_file",
                "message": "è¯»å– README.md æ–‡ä»¶å†…å®¹"
            },
            "tool_names": ["Read"],
            "final_output": "README.md æ–‡ä»¶å†…å®¹å·²è¯»å–ã€‚",
            "passed": True,
            "duration_s": 0.8
        },
        {
            "case": {
                "id": "test_complex_task",
                "message": "åˆ†æä»£ç å¹¶ç”ŸæˆæŠ¥å‘Š"
            },
            "tool_names": ["Glob", "Read", "Grep", "Write"],
            "final_output": "å·²å®Œæˆä»£ç åˆ†æï¼ŒæŠ¥å‘Šå·²ç”Ÿæˆã€‚",
            "passed": True,
            "duration_s": 5.3
        }
    ]

    report_path = Path("test_integration_report.json")
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(test_report, f, indent=2, ensure_ascii=False)
    print(f"âœ“ æµ‹è¯•æŠ¥å‘Šå·²åˆ›å»º: {report_path}")

    # 2. éªŒè¯æ•°æ®ç»“æ„
    print("\n[2/5] éªŒè¯æ•°æ®ç»“æ„...")
    for i, result in enumerate(test_report, 1):
        assert "case" in result, f"æµ‹è¯• {i} ç¼ºå°‘ case å­—æ®µ"
        assert "id" in result["case"], f"æµ‹è¯• {i} ç¼ºå°‘ case.id"
        assert "message" in result["case"], f"æµ‹è¯• {i} ç¼ºå°‘ case.message"
        assert "tool_names" in result, f"æµ‹è¯• {i} ç¼ºå°‘ tool_names"
        assert "final_output" in result, f"æµ‹è¯• {i} ç¼ºå°‘ final_output"
        print(f"âœ“ æµ‹è¯•ç”¨ä¾‹ {result['case']['id']} æ•°æ®ç»“æ„æ­£ç¡®")

    # 3. æ¨¡æ‹Ÿ LLM è¯„ä¼°ï¼ˆä¸å®é™…è°ƒç”¨ APIï¼‰
    print("\n[3/5] æ¨¡æ‹Ÿ LLM è¯„ä¼°...")
    judged_results = []

    for result in test_report:
        case_id = result["case"]["id"]
        tool_count = len(result["tool_names"])

        # æ ¹æ®å·¥å…·æ•°é‡å’Œå¤æ‚åº¦ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æ•°
        if tool_count == 1:
            scores = {
                "tool_selection_score": 7,
                "tool_order_score": 8,
                "output_quality_score": 8,
                "overall_score": 8
            }
        elif tool_count <= 2:
            scores = {
                "tool_selection_score": 8,
                "tool_order_score": 8,
                "output_quality_score": 9,
                "overall_score": 8
            }
        else:
            scores = {
                "tool_selection_score": 9,
                "tool_order_score": 9,
                "output_quality_score": 9,
                "overall_score": 9
            }

        result_copy = result.copy()
        result_copy["llm_judgment"] = {
            **scores,
            "reasoning": f"å·¥å…·é€‰æ‹©åˆç†ï¼ˆ{tool_count} ä¸ªå·¥å…·ï¼‰ï¼Œè¾“å‡ºè´¨é‡è‰¯å¥½",
            "model": "claude-sonnet-4-5-20250929",
            "note": "è¿™æ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…ä½¿ç”¨éœ€è¦è®¾ç½® ANTHROPIC_API_KEY"
        }
        judged_results.append(result_copy)

        print(f"âœ“ {case_id}: ç»¼åˆå¾—åˆ† {scores['overall_score']}/10")

    # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    print("\n[4/5] è®¡ç®—ç»Ÿè®¡ä¿¡æ¯...")
    avg_overall = sum(r["llm_judgment"]["overall_score"] for r in judged_results) / len(judged_results)
    avg_tool_selection = sum(r["llm_judgment"]["tool_selection_score"] for r in judged_results) / len(judged_results)
    avg_tool_order = sum(r["llm_judgment"]["tool_order_score"] for r in judged_results) / len(judged_results)
    avg_output_quality = sum(r["llm_judgment"]["output_quality_score"] for r in judged_results) / len(judged_results)

    print("â”€" * 60)
    print("ğŸ“Š è¯„ä¼°ç»Ÿè®¡")
    print("â”€" * 60)
    print(f"å¹³å‡ç»¼åˆå¾—åˆ†: {avg_overall:.1f}/10")
    print(f"å¹³å‡å·¥å…·é€‰æ‹©: {avg_tool_selection:.1f}/10")
    print(f"å¹³å‡å·¥å…·é¡ºåº: {avg_tool_order:.1f}/10")
    print(f"å¹³å‡è¾“å‡ºè´¨é‡: {avg_output_quality:.1f}/10")

    # 5. ä¿å­˜ç»“æœ
    print("\n[5/5] ä¿å­˜è¯„ä¼°ç»“æœ...")
    output_path = Path("test_integration_report.judged.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(judged_results, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")

    # éªŒè¯è¾“å‡ºæ–‡ä»¶
    with open(output_path, 'r', encoding='utf-8') as f:
        loaded = json.load(f)
        assert len(loaded) == len(test_report), "è¾“å‡ºæ–‡ä»¶è®°å½•æ•°ä¸åŒ¹é…"
        for r in loaded:
            assert "llm_judgment" in r, "è¾“å‡ºç¼ºå°‘ llm_judgment å­—æ®µ"
            assert "overall_score" in r["llm_judgment"], "è¾“å‡ºç¼ºå°‘ overall_score"

    print("\n" + "=" * 60)
    print("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)

    print("\nğŸ“ ä½¿ç”¨è¯´æ˜ï¼š")
    print("1. è¿™æ˜¯æ¨¡æ‹Ÿæµ‹è¯•ï¼ŒéªŒè¯äº† judge å‘½ä»¤çš„æ•°æ®æµç¨‹")
    print("2. å®é™…ä½¿ç”¨éœ€è¦è®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡")
    print("3. å‘½ä»¤ç¤ºä¾‹ï¼š")
    print("   export ANTHROPIC_API_KEY=your_key")
    print("   edd edd judge --report test_integration_report.json")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    report_path.unlink()
    output_path.unlink()
    print("âœ“ æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")

if __name__ == "__main__":
    try:
        test_judge_workflow()
    except Exception as e:
        print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
